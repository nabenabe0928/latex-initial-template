####################################################
###### section 2: Evaluation
####################################################
% fighting overfit
@article{nakkiran2019deep,
  title   = {Deep double descent: Where bigger models and more data hurt},
  author  = {P. Nakkiran and G. Kaplun and Y. Bansal and T. Yang and B. Barak and I. Sutskever},
  journal = {arXiv:1912.02292},
  year    = {2019}
}

% a set of statistical tests, overcome the data overlaps in cross validation
@article{dietterich1998approximate,
  title   = {Approximate statistical tests for comparing supervised classification learning algorithms},
  author  = {TG. Dietterich},
  journal = {Neural Computation},
  volume  = {10},
  year    = {1998}
}

####################################################
###### section 3: Algorithm selection/configuration
####################################################
% The problem setting of algorithm selection, key ideas: recommended paper
@incollection{rice1976algorithm,
  title     = {The algorithm selection problem},
  author    = {JR. Rice},
  booktitle = {Advances in Computers},
  volume    = {15},
  year      = {1976}
}

% performance differences
@inproceedings{strang2018don,
  title     = {Donâ€™t rule out simple models prematurely: a large scale benchmark comparing linear and non-linear classifiers in {OpenML}},
  author    = {B. Strang and P. van der Putten and JN. van Rijn and F. Hutter},
  booktitle = {International Symposium on Intelligent Data Analysis},
  year      = {2018}
}

% parallel portfolio
@inproceedings{lindauer2015sequential,
  title     = {From sequential algorithm selection to parallel portfolio selection},
  author    = {M. Lindauer and H. Hoos and F. Hutter},
  booktitle = {International Conference on Learning and Intelligent Optimization},
  year      = {2015}
}

% instance weighting by a cost-sensitive metric
@article{ting2002instance,
  title   = {An instance-weighting method to induce cost-sensitive trees},
  author  = {KM. Ting},
  journal = {Transactions on Knowledge and Data Engineering},
  volume  = {14},
  year    = {2002}
}

% instance weighting: application
@inproceedings{xu2011hydra,
  title     = {{Hydra-MIP}: Automated algorithm configuration and selection for mixed integer programming},
  author    = {L. Xu and F. Hutter and H. Hoos H and K. Leyton-Brown},
  booktitle = {RCRA Workshop at International Joint Conference on Artificial Intelligence},
  year      = {2011}
}

@inproceedings{kleinberg2017efficiency,
  title     = {Efficiency Through Procrastination: Approximately Optimal Algorithm Configuration with Runtime Guarantees},
  author    = {R. Kleinberg and K. Leyton-Brown and B. Lucier},
  booktitle = {International Joint Conference on Artificial Intelligence},
  year      = {2017}
}

@article{eggensperger2019pitfalls,
  title   = {Pitfalls and best practices in algorithm configuration},
  author  = {K. Eggensperger and M. Lindauer and F. Hutter},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {64},
  year    = {2019}
}

@article{hutter2009paramils,
  title   = {{ParamILS}: an automatic algorithm configuration framework},
  author  = {F. Hutter and H. Hoos H and K. Leyton-Brown and T. St{\"u}tzle},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {36},
  year    = {2009}
}

@incollection{kadioglu2010isac,
  title     = {{ISAC}--instance-specific algorithm configuration},
  author    = {S. Kadioglu and Y. Malitsky and M. Sellmann and K. Tierney},
  booktitle = {European Conference on Artificial Intelligence},
  year      = {2010}
}

@inproceedings{liu2019automatic,
  title     = {Automatic construction of parallel portfolios via explicit instance grouping},
  author    = {S. Liu and K. Tang and X. Yao},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year      = {2019}
}

@inproceedings{seipp2015automatic,
  title     = {Automatic configuration of sequential planning portfolios},
  author    = {J. Seipp and S. Sievers and M. Helmert and F. Hutter},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year      = {2015}
}

####################################################
###### section 4: hyperparameter optimization
####################################################
% extensive survey of HPO
@incollection{feurer2019hyperparameter,
  title     = {Hyperparameter optimization},
  author    = {M. Feurer and F. Hutter},
  booktitle = {Automated Machine Learning},
  pages     = {3--33},
  year      = {2019},
  publisher = {Springer}
}

% the importance of the hyperparameter optimization
@article{chen2018bayesian,
  title   = {{Bayesian} optimization in {AlphaGo}},
  author  = {Y. Chen and A. Huang and Z. Wang and I. Antonoglou and J. Schrittwieser and D. Silver and N. de Freitas},
  journal = {arXiv:1812.06855},
  year    = {2018}
}

% random search vs grid search
@article{bergstra2012random,
  title   = {Random search for hyper-parameter optimization},
  author  = {J. Bergstra and Y. Bengio},
  journal = {Journal of Machine Learning Research},
  volume  = {13},
  number  = {2},
  year    = {2012}
}

% mutation for numeric parameters
@article{deb2014analysing,
  title   = {Analysing mutation schemes for real-parameter genetic algorithms},
  author  = {K. Deb and D. Deb},
  journal = {International Journal of Artificial Intelligence and Soft Computing},
  volume  = {4},
  year    = {2014}
}

% hyper gradient
@inproceedings{maclaurin2015gradient,
  title     = {Gradient-based hyperparameter optimization through reversible learning},
  author    = {D. Maclaurin and D. Duvenaud and R. Adams},
  booktitle = {International Conference on Machine Learning},
  year      = {2015}
}

% Spearmint
@article{snoek2012practical,
  title   = {Practical {Bayesian} optimization of machine learning algorithms},
  author  = {J. Snoek and H. Larochelle and RP. Adams},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2012}
}

% SMAC (hamming kernel)
@inproceedings{hutter2011sequential,
  title     = {Sequential model-based optimization for general algorithm configuration},
  author    = {F. Hutter and H. Hoos H and K. Leyton-Brown},
  booktitle = {International Conference on Learning and Intelligent Optimization},
  year      = {2011}
}

% TPE
@article{bergstra2011algorithms,
  title   = {Algorithms for hyper-parameter optimization},
  author  = {J. Bergstra and R. Bardenet and Y. Bengio and B. K{\'e}gl},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2011}
}

% TPE 2
@inproceedings{bergstra2013making,
  title     = {Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures},
  author    = {J. Bergstra and D. Yamins and D. Cox},
  booktitle = {International Conference on Machine Learning},
  year      = {2013}
}

% optuna
@inproceedings{akiba2019optuna,
  title     = {{Optuna}: A next-generation hyperparameter optimization framework},
  author    = {T. Akiba and S. Sano and T. Yanase and T. Ohta and M. Koyama},
  booktitle = {International Conference on Knowledge Discovery \& Data Mining},
  year      = {2019}
}

% nelder-mead
@article{nelder1965simplex,
  title   = {A simplex method for function minimization},
  author  = {JA. Nelder and R. Mead},
  journal = {Computer Journal},
  volume  = {7},
  year    = {1965}
}

% Genetic algorithm
@article{whitley1994genetic,
  title   = {A genetic algorithm tutorial},
  author  = {D. Whitley},
  journal = {Statistics and Computing},
  volume  = {4},
  year    = {1994}
}

% CMA-ES
@article{hansen2016cma,
  title   = {The {CMA} evolution strategy: A tutorial},
  author  = {N. Hansen},
  journal = {arXiv:1604.00772},
  year    = {2016}
}

% CMA-ES for HPO
@article{loshchilov2016cma,
  title   = {{CMA-ES} for hyperparameter optimization of deep neural networks},
  author  = {I. Loshchilov and F. Hutter},
  journal = {arXiv:1604.07269},
  year    = {2016}
}

% pycma
@article{hansen2019cma,
  title   = {{CMA-ES}/pycma on {GitHub}},
  author  = {N. Hansen and Y. Akimoto and P. Baudis},
  journal = {Zenodo, doi},
  volume  = {10},
  year    = {2019}
}

% differential evolution
@inproceedings{storn1996usage,
  title     = {On the usage of differential evolution for function optimization},
  author    = {R. Storn},
  booktitle = {North American Fuzzy Information Processing},
  year      = {1996}
}

####################################################
###### section 5: Gaussian process
####################################################
% details of GP
@book{williams2006gaussian,
  title     = {Gaussian processes for machine learning},
  author    = {CK. Williams and CE. Rasmussen},
  volume    = {2},
  year      = {2006},
  publisher = {MIT press}
}

% Nystroem original
@article{williams2001using,
  title     = {Using the {Nystr{\"o}m} method to speed up kernel machines},
  author    = {C. Williams and M. Seeger},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2001}
}

% Large-Scale Nystroem Approximation
@inproceedings{affandi2013nystrom,
  title     = {{Nystr{\"o}m} approximation for large-scale determinantal processes},
  author    = {RH. Affandi and A. Kulesza and E. Fox and B. Taskar},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  year      = {2013}
}

% bayesian committee
@article{tresp2000bayesian,
  title   = {A {Bayesian} committee machine},
  author  = {V. Tresp},
  journal = {Neural Computation},
  volume  = {12},
  year    = {2000}
}

####################################################
###### section 6: Bayesian optimization
####################################################

% Bayesian optimization: the original paper
@article{mockus1978application,
  title   = {The application of {Bayesian} methods for seeking the extremum},
  author  = {J. Mockus and V. Tiesis and A. Zilinskas},
  journal = {Towards Global Optimization},
  volume  = {2},
  year    = {1978}
}

% Bayesian neural networks (original paper)
@inproceedings{blundell2015weight,
  title     = {Weight uncertainty in neural network},
  author    = {C. Blundell and J. Cornebise and K. Kavukcuoglu and D. Wierstra},
  booktitle = {International Conference on Machine Learning},
  year      = {2015}
}

% BOHAMIANN (BO with BNN)
@article{springenberg2016bayesian,
  title   = {{Bayesian} optimization with robust {Bayesian} neural networks},
  author  = {JT. Springenberg and A. Klein and S. Falkner and F. Hutter},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2016}
}

% BO with BNN
@inproceedings{hernandez2017parallel,
  title     = {Parallel and distributed {Thompson} sampling for large-scale accelerated exploration of chemical space},
  author    = {JM. Hern{\'a}ndez-Lobato and J. Requeima and EO. Pyzer-Knapp and A. Aspuru-Guzik},
  booktitle = {International Conference on Machine Learning},
  year      = {2017}
}

% BO with BNN
@inproceedings{schilling2015hyperparameter,
  title     = {Hyperparameter optimization with factorized multilayer perceptrons},
  author    = {N. Schilling and M. Wistuba and L. Drumond and L. Schmidt-Thieme},
  booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  year      = {2015}
}

% DNGO
@inproceedings{snoek2015scalable,
  title     = {Scalable {Bayesian} optimization using deep neural networks},
  author    = {J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and M. Prabhat and R. Adams},
  booktitle = {International Conference on Machine Learning},
  year      = {2015}
}

% Convergence analysis, UCB scheduling
@article{srinivas2009gaussian,
  title   = {Gaussian process optimization in the bandit setting: No regret and experimental design},
  author  = {N. Srinivas and A. Krause and SM. Kakade and M. Seeger},
  journal = {arXiv:0912.3995},
  year    = {2009}
}

% Convergence analysis
@article{bull2011convergence,
  title   = {Convergence rates of efficient global optimization algorithms},
  author  = {AD. Bull},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  year    = {2011}
}

% Convergence analysis
@article{de2012exponential,
  title   = {Exponential regret bounds for {Gaussian} process bandits with deterministic observations},
  author  = {N. de Freitas and A. Smola and M. Zoghi},
  journal = {arXiv:1206.6457},
  year    = {2012}
}

% Convergence analysis 
@article{kawaguchi2015bayesian,
  title   = {{Bayesian} optimization with exponential convergence},
  author  = {K. Kawaguchi and LP. Kaelbling and T. Lozano-P{\'e}rez},
  year    = {2015},
  journal = {Advances in Neural Information Processing Systems}
}

% expected improvement
@article{jones1998efficient,
  title     = {Efficient global optimization of expensive black-box functions},
  author    = {DR. Jones and M. Schonlau and WJ. Welch},
  journal   = {Journal of Global Optimization},
  volume    = {13},
  year      = {1998},
}

% thompson sampling tutorial
@article{russo2017tutorial,
  title   = {A tutorial on {Thompson} sampling},
  author  = {D. Russo and B. Van Roy and A. Kazerouni and I. Osband and Z. Wen},
  journal = {arXiv:1707.02038},
  year    = {2017}
}

% thompson sampling: original
@article{thompson1933likelihood,
  title     = {On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
  author    = {WR. Thompson},
  journal   = {Biometrika},
  volume    = {25},
  year      = {1933},
}

% probablity improvement
@article{kushner1964new,
  title  = {A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise},
  author = {HJ. Kushner},
  year   = {1964}
}

% efficient knowledge gradient, tutorial on BO
@article{frazier2018tutorial,
  title   = {A tutorial on {Bayesian} optimization},
  author  = {P. Frazier},
  journal = {arXiv:1807.02811},
  year    = {2018}
}

% lookahead bayesian optimization
@article{wu2019practical,
  title   = {Practical two-step lookahead {Bayesian} optimization},
  author  = {J. Wu and P. Frazier},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2019}
}

% knowledge gradient
@article{wu2016parallel,
  title   = {The parallel knowledge gradient method for batch {Bayesian} optimization},
  author  = {J. Wu and P. Frazier},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2016}
}

% entropy search (frequently used version)
@article{hernandez2014predictive,
  title   = {Predictive entropy search for efficient global optimization of black-box functions},
  author  = {JM. Hern{\'a}ndez-Lobato and MW. Hoffman and Z. Ghahramani},
  journal = {arXiv:1406.2541},
  year    = {2014}
}

% entropy search (original paper)
@article{hennig2012entropy,
  title   = {Entropy Search for Information-Efficient Global Optimization},
  author  = {P. Hennig and CJ. Schuler},
  journal = {Journal of Machine Learning Research},
  volume  = {13},
  year    = {2012}
}

% entropy search (recent variant)
@inproceedings{wang2017max,
  title        = {Max-value entropy search for efficient {Bayesian} optimization},
  author       = {Z. Wang and S. Jegelka},
  booktitle    = {International Conference on Machine Learning},
  year         = {2017},
}

% entropy search: summary
@inproceedings{metzen2016minimum,
  title     = {Minimum regret search for single-and multi-task optimization},
  author    = {JH. Metzen},
  booktitle = {International Conference on Machine Learning},
  year      = {2016}
}

% thompson sampling for Bayesian optimization
@inproceedings{kandasamy2018parallelised,
  title     = {Parallelised {Bayesian} optimisation via {Thompson} sampling},
  author    = {K. Kandasamy and A. Krishnamurthy and J. Schneider and B. P{\'o}czos},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  year      = {2018}
}

% entity embeddings of categorical parameters
@article{guo2016entity,
  title   = {Entity embeddings of categorical variables},
  author  = {C. Guo and F. Berkhahn},
  journal = {arXiv:1604.06737},
  year    = {2016}
}

% conditional kernel
@inproceedings{levesque2017bayesian,
  title        = {{Bayesian} optimization for conditional hyperparameter spaces},
  author       = {JC. L{\'e}vesque and A. Durand and C. Gagn{\'e} and R. Sabourin},
  booktitle    = {International Joint Conference on Neural Networks},
  year         = {2017},
}

% conditional kernel
@article{hutter2013kernel,
  title   = {A kernel for hierarchical parameter spaces},
  author  = {F. Hutter and MA. Osborne},
  journal = {arXiv:1310.5738},
  year    = {2013}
}

% conditional kernel
@inproceedings{jenatton2017bayesian,
  title        = {{Bayesian} optimization with tree-structured dependencies},
  author       = {R. Jenatton and C. Archambeau and J. Gonz{\'a}lez and M. Seeger},
  booktitle    = {International Conference on Machine Learning},
  year         = {2017},
}

% high dimensional Bayesian optimization by random embedding
@article{wang2016bayesian,
  title   = {{Bayesian} optimization in a billion dimensions via random embeddings},
  author  = {Z. Wang and F. Hutter and M. Zoghi and D. Matheson and N. de Feitas},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {55},
  year    = {2016}
}

% high dimensional Bayesian optimization by additive models
@inproceedings{kandasamy2015high,
  title        = {High dimensional {Bayesian} optimisation and bandits via additive models},
  author       = {K. Kandasamy and J. Schneider and B. P{\'o}czos},
  booktitle    = {International Conference on Machine Learning},
  year         = {2015},
}

% parallel Bayesian optimization
@article{ginsbourger2008multi,
  title  = {A multi-points criterion for deterministic parallel global optimization based on {Gaussian} processes},
  author = {D. Ginsbourger and R. Le Riche and L. Carraro},
  year   = {2008}
}

% parallel Bayesian optimization, q-EI
@article{wang2016parallel,
  title   = {Parallel {Bayesian} global optimization of expensive functions},
  author  = {J. Wang and SC. Clark and E. Liu and P. Frazier},
  journal = {arXiv:1602.05149},
  year    = {2016}
}

% parallel Bayesian optimization
% another source --> wu2016parallel

% optimization of the multiple-points acquisition function by gradient descent using reparametrization trick
@article{wilson2018maximizing,
  title   = {Maximizing acquisition functions for {Bayesian} optimization},
  author  = {JT. Wilson and F. Hutter and MP. Deisenroth},
  journal = {arXiv:1805.10196},
  year    = {2018}
}

% parallel sampling using kriging and constant liar
@incollection{ginsbourger2010kriging,
  title     = {Kriging is well-suited to parallelize optimization},
  author    = {D. Ginsbourger and R. Le Riche and L. Carraro},
  booktitle = {Computational Intelligence in Expensive Optimization Problems},
  pages     = {131--162},
  year      = {2010},
  publisher = {Springer}
}

% expected constraint improvement
@article{lee2011optimization,
  title   = {Optimization subject to hidden constraints via statistical emulation},
  author  = {H. Lee and R. Gramacy and C. Linkletter and G. Gray},
  journal = {Pacific Journal of Optimization},
  volume  = {7},
  year    = {2011}
}

% BO with unknown constraint
@article{gelbart2014bayesian,
  title   = {{Bayesian} optimization with unknown constraints},
  author  = {MA. Gelbart and J. Snoek and RP. Adams},
  journal = {arXiv:1403.5607},
  year    = {2014}
}

% BO with inequality constraints
@inproceedings{gardner2014bayesian,
  title     = {{Bayesian} Optimization with Inequality Constraints},
  author    = {JR. Gardner and MJ. Kusner and ZE. Xu and KQ. Weinberger and JP. Cunningham},
  booktitle = {International Conference on Machine Learning},
  year      = {2014}
}

% BO with noisy experiments
@article{letham2019constrained,
  title     = {Constrained {Bayesian} optimization with noisy experiments},
  author    = {B. Letham and B. Karrer and G. Ottoni and E. Bakshy},
  journal   = {Bayesian Analysis},
  volume    = {14},
  year      = {2019},
}

% kernel chemical structure
@article{griffiths2017constrained,
  title   = {Constrained {Bayesian} optimization for automatic chemical design},
  author  = {RR. Griffiths and JM. Hern{\'a}ndez-Lobato},
  journal = {arXiv:1709.05501},
  year    = {2017}
}

% the combination of Bayesian optimization and local search
@article{eriksson2019scalable,
  title   = {Scalable global optimization via local {Bayesian} optimization},
  author  = {D. Eriksson and M. Pearce and J. Gardner and RD. Turner and M. Poloczek},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2019}
}

####################################################
###### section 7: multi-fidelity optimization
####################################################
% extensive survey about meta-learning
@incollection{vanschoren2019meta,
  title     = {Meta-learning},
  author    = {J. Vanschoren},
  booktitle = {Automated Machine Learning},
  pages     = {35--61},
  year      = {2019},
  publisher = {Springer}
}

% ensemble, warm-starting
@article{feurer2018scalable,
  title   = {Scalable meta-learning for {Bayesian} optimization},
  author  = {M. Feurer and B. Letham and E. Bakshy},
  journal = {AutoML Workshop at International Conference on Machine Learning},
  volume  = {1050},
  pages   = {6},
  year    = {2018}
}

% task-independent recommendation
@article{abdulrahman2018speeding,
  title     = {Speeding up algorithm selection using average ranking and active testing by introducing runtime},
  author    = {Abdulrahman, Salisu Mamman and Brazdil, Pavel and JN. van Rijn and Vanschoren, Joaquin},
  journal   = {Machine learning},
  volume    = {107},
  number    = {1},
  pages     = {79--108},
  year      = {2018},
  publisher = {Springer}
}

% task-independent recommendation
@article{wistuba2018scalable,
  title     = {Scalable {Gaussian} process-based transfer surrogates for hyperparameter optimization},
  author    = {M. Wistuba and N. Schilling and L. Schmidt-Thieme},
  journal   = {Machine Learning},
  volume    = {107},
  number    = {1},
  pages     = {43--78},
  year      = {2018},
  publisher = {Springer}
}

% task-independent recommendation, meta learn for large scale HPO
@article{feurer2018practical,
  title   = {Practical Transfer Learning for {Bayesian} Optimization},
  author  = {M. Feurer and B. Letham and F. Hutter and Bakshy, Eytan},
  journal = {arXiv:1802.02219},
  year    = {2018}
}

% warm-start CMA-ES
@article{nomura2020warm,
  title   = {Warm Starting {CMA-ES} for Hyperparameter Optimization},
  author  = {Nomura, Masahiro and Watanabe, Shuhei and Akimoto, Youhei and Ozaki, Yoshihiko and Onishi, Masaki},
  journal = {arXiv:2012.06932},
  year    = {2020}
}

% multi-task bayesian optimization
@article{swersky2013multi,
  title     = {Multi-task {Bayesian} optimization},
  author    = {K. Swersky and J. Snoek and RP. Adams},
  year      = {2013},
  publisher = {Curran Associates, Inc}
}

% multi-task gaussian process
@article{williams2007multi,
  title   = {Multi-task {Gaussian} process prediction},
  author  = {Williams, Chris and Bonilla, Edwin V and Chai, Kian M},
  journal = {Advances in Neural Information Processing Systems},
  pages   = {153--160},
  year    = {2007}
}

% joint optimization over multiple task using shared task latent representation
@inproceedings{perrone2018scalable,
  title     = {Scalable hyperparameter transfer learning},
  author    = {Perrone, Valerio and Jenatton, Rodolphe and M. Seeger and Archambeau, C{\'e}dric},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages     = {6846--6856},
  year      = {2018}
}

% FABOLAS, Entropy search, multi-fidelity bayesian optimization
@inproceedings{klein2017fast,
  title        = {Fast {Bayesian} optimization of machine learning hyperparameters on large datasets},
  author       = {A. Klein and S. Falkner and Bartels, Simon and Hennig, Philipp and F. Hutter},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  pages        = {528--536},
  year         = {2017},
  organization = {PMLR}
}

% multi-fidelity bayesian optimization
@inproceedings{kandasamy2017multi,
  title        = {Multi-fidelity {Bayesian} optimisation with continuous approximations},
  author       = {K. Kandasamy and Dasarathy, Gautam and J. Schneider and B. P{\'o}czos},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1799--1808},
  year         = {2017},
  organization = {PMLR}
}

% successive halving
@inproceedings{jamieson2016non,
  title        = {Non-stochastic best arm identification and hyperparameter optimization},
  author       = {Jamieson, Kevin and Talwalkar, Ameet},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  pages        = {240--248},
  year         = {2016},
  organization = {PMLR}
}

% hyperband
@article{li2017hyperband,
  title     = {{HyperBand}: A novel bandit-based approach to hyperparameter optimization},
  author    = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal   = {Journal of Machine Learning Research},
  volume    = {18},
  number    = {1},
  pages     = {6765--6816},
  year      = {2017},
  publisher = {JMLR. org}
}

% BOHB
@inproceedings{falkner2018bohb,
  title        = {{BOHB}: Robust and efficient hyperparameter optimization at scale},
  author       = {S. Falkner and A. Klein and F. Hutter},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1437--1446},
  year         = {2018},
  organization = {PMLR}
}

% learning of acquisition function
@article{volpp2019meta,
  title   = {Meta-learning acquisition functions for transfer learning in {Bayesian} optimization},
  author  = {Volpp, Michael and Fr{\"o}hlich, Lukas P and Fischer, Kirsten and Doerr, Andreas and S. Falkner and F. Hutter and Daniel, Christian},
  journal = {arXiv:1904.02642},
  year    = {2019}
}

% learning a black box optimization by gradient descent
@inproceedings{andrychowicz2016learning,
  title     = {Learning to learn by gradient descent by gradient descent},
  author    = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and N. de Freitas},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {3981--3989},
  year      = {2016}
}

% learning a black box optimization (acquisition function) by gradient descent
@inproceedings{chen2017learning,
  title        = {Learning to learn without gradient descent by gradient descent},
  author       = {Y. Chen and Hoffman, Matthew W and Colmenarejo, Sergio G{\'o}mez and Denil, Misha and Lillicrap, Timothy P and Botvinick, Matt and Freitas, Nando},
  booktitle    = {International Conference on Machine Learning},
  pages        = {748--756},
  year         = {2017},
  organization = {PMLR}
}

% learning a black box optimization by reinforcement learning
@article{li2016learning,
  title   = {Learning to optimize},
  author  = {Li, Ke and Malik, Jitendra},
  journal = {arXiv:1606.01885},
  year    = {2016}
}

% learning curve prediction by simple model
@inproceedings{chandrashekaran2017speeding,
  title        = {Speeding up hyper-parameter optimization by extrapolation of learning curves using previous builds},
  author       = {Chandrashekaran, Akshay and Lane, Ian R},
  booktitle    = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages        = {477--492},
  year         = {2017},
  organization = {Springer}
}

% learning curve prediction using neural network
@article{klein2016learning,
  title  = {Learning curve prediction with {Bayesian} neural networks},
  author = {A. Klein and S. Falkner and JT. Springenberg and F. Hutter},
  year   = {2016}
}

% learning curve prediction by parametric non-linear regression
@inproceedings{domhan2015speeding,
  title     = {Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves},
  author    = {Domhan, Tobias and JT. Springenberg and F. Hutter},
  booktitle = {International Joint Conference on Artificial Intelligence},
  year      = {2015}
}

% learning curve prediction using a kernel for budgeting
@article{swersky2014freeze,
  title   = {Freeze-thaw {Bayesian} optimization},
  author  = {K. Swersky and J. Snoek and Adams, Ryan Prescott},
  journal = {arXiv:1406.3896},
  year    = {2014}
}

% auto-sklearn
@incollection{feurer2019auto,
  title     = {Auto-sklearn: efficient and robust Automated Machine Learning},
  author    = {M. Feurer and A. Klein and K. Eggensperger and JT. Springenberg and M. Blum and F. Hutter},
  booktitle = {Automated Machine Learning},
  pages     = {113--134},
  year      = {2019},
  publisher = {Springer}
}

% auto-sklearn 2.0
@article{feurer2020auto,
  title   = {Auto-sklearn 2.0: The next generation},
  author  = {M. Feurer and K. Eggensperger and S. Falkner and M. Lindauer and F. Hutter},
  journal = {arXiv:2007.04074},
  year    = {2020}
}

% meta-learn for auto-sklearn
@inproceedings{feurer2015initializing,
  title     = {Initializing {Bayesian} hyperparameter optimization via meta-learning},
  author    = {M. Feurer and Springenberg, Jost and F. Hutter},
  booktitle = {AAAI Conference on Artificial Intelligence},
  volume    = {29},
  number    = {1},
  year      = {2015}
}

% Auto-weka
@inproceedings{thornton2013auto,
  title     = {{Auto-WEKA}: Combined selection and hyperparameter optimization of classification algorithms},
  author    = {Thornton, Chris and F. Hutter and H. Hoos H and K. Leyton-Brown},
  booktitle = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages     = {847--855},
  year      = {2013}
}

####################################################
###### section 8: multi-objective optimization
####################################################
% HPO joinly with feature selection
@inproceedings{binder2020multi,
  title     = {Multi-objective hyperparameter tuning and feature selection using filter ensembles},
  author    = {Binder, Martin and Moosbauer, Julia and Thomas, Janek and Bischl, Bernd},
  booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  pages     = {471--479},
  year      = {2020}
}

% SMS-EMOA
@article{beume2007sms,
  title     = {{SMS-EMOA}: Multiobjective selection based on dominated hypervolume},
  author    = {Beume, Nicola and Naujoks, Boris and Emmerich, Michael},
  journal   = {European Journal of Operational Research},
  volume    = {181},
  number    = {3},
  pages     = {1653--1669},
  year      = {2007},
  publisher = {Elsevier}
}

% NSGA-II
@article{deb2002fast,
  title     = {A fast and elitist multiobjective genetic algorithm: {NSGA-II}},
  author    = {K. Deb and Pratap, Amrit and Agarwal, Sameer and Meyarivan, TAMT},
  journal   = {IEEE transactions on evolutionary computation},
  volume    = {6},
  number    = {2},
  pages     = {182--197},
  year      = {2002},
  publisher = {IEEE}
}

% Non-dominated sorting: O(DN log N) ~ O(DN^2)
@article{xue2020setnds,
  title     = {{SETNDS}: A {SET}-Based {N}on-{D}ominated Sorting Algorithm for Multi-Objective Optimization Problems},
  author    = {Xue, Lingling and Zeng, Peng and Yu, Haibin},
  journal   = {Applied Sciences},
  volume    = {10},
  number    = {19},
  pages     = {6858},
  year      = {2020},
  publisher = {Multidisciplinary Digital Publishing Institute}
}

% ParEGO (scalarization of metrics)
@article{knowles2006parego,
  title     = {{ParEGO}: A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems},
  author    = {Knowles, Joshua},
  journal   = {IEEE Transactions on Evolutionary Computation},
  volume    = {10},
  number    = {1},
  pages     = {50--66},
  year      = {2006},
  publisher = {IEEE}
}

% SMS-EGO
@inproceedings{ponweiser2008multiobjective,
  title        = {Multiobjective optimization on a limited budget of evaluations using model-assisted {S}-metric selection},
  author       = {Ponweiser, Wolfgang and Wagner, Tobias and Biermann, Dirk and Vincze, Markus},
  booktitle    = {International Conference on Parallel Problem Solving from Nature},
  pages        = {784--794},
  year         = {2008},
  organization = {Springer}
}

% Expected hypervolume improvement
@article{yang2019multi,
  title     = {Multi-objective {Bayesian} global optimization using expected hypervolume improvement gradient},
  author    = {Yang, Kaifeng and Emmerich, Michael and Deutz, Andr{\'e} and B{\"a}ck, Thomas},
  journal   = {Swarm and evolutionary computation},
  volume    = {44},
  pages     = {945--956},
  year      = {2019},
  publisher = {Elsevier}
}

% multi-objective TPE
@inproceedings{ozaki2020multiobjective,
  title     = {Multiobjective tree-structured {Parzen} estimator for computationally expensive optimization problems},
  author    = {Ozaki, Yoshihiko and Tanigaki, Yuki and Watanabe, Shuhei and Onishi, Masaki},
  booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  pages     = {533--541},
  year      = {2020}
}

####################################################
###### section 9, 10: neural architecture search
####################################################
% Best practices for NAS
@article{lindauer2020best,
  title   = {Best practices for scientific research on neural architecture search},
  author  = {M. Lindauer and F. Hutter},
  journal = {Journal of Machine Learning Research},
  volume  = {21},
  number  = {243},
  pages   = {1--18},
  year    = {2020}
}

% Network morphism: original paper
@inproceedings{wei2016network,
  title        = {Network morphism},
  author       = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
  booktitle    = {International Conference on Machine Learning},
  pages        = {564--572},
  year         = {2016},
  organization = {PMLR}
}

% Auto-pytorch: first version
@inproceedings{mendoza2016towards,
  title     = {Towards automatically-tuned neural networks},
  author    = {Mendoza, Hector and A. Klein and M. Feurer and JT. Springenberg and F. Hutter},
  booktitle = {AutoML Workshop at International Conference on Machine Learning},
  year      = {2016}
}

% Auto pytorch: second version
@article{zimmer2021auto,
  title   = {{Auto-PyTorch}: Multi-Fidelity MetaLearning for Efficient and Robust {AutoDL}},
  author  = {L. Zimmer and M. Lindauer and F. Hutter},
  journal = {Transactions on Pattern Analysis and Machine Intelligence},
  year    = {2021}
}

% NAS-bench101
@inproceedings{ying2019bench,
  title        = {{NAS-Bench-101}: Towards reproducible neural architecture search},
  author       = {C. Ying and A. Klein and Christiansen, Eric and Real, Esteban and Murphy, Kevin and F. Hutter},
  booktitle    = {International Conference on Machine Learning},
  pages        = {7105--7114},
  year         = {2019},
  organization = {PMLR}
}

% NAS-Bench201
@article{dong2020bench,
  title   = {{NAS-Bench-201}: Extending the scope of reproducible neural architecture search},
  author  = {Dong, Xuanyi and Yang, Yi},
  journal = {arXiv:2001.00326},
  year    = {2020}
}

% NAS-Bench301
@article{siems2020bench,
  title   = {{NAS-Bench-301} and the case for surrogate benchmarks for neural architecture search},
  author  = {J. Siems and L. Zimmer and A. Zela and Lukasik, Jovita and Keuper, Margret and F. Hutter},
  journal = {arXiv:2008.09777},
  year    = {2020}
}

% NAS-Bench-1shot1
@article{zela2020bench,
  title   = {{NAS-Bench-1Shot1}: Benchmarking and dissecting one-shot neural architecture search},
  author  = {A. Zela and J. Siems and F. Hutter},
  journal = {arXiv:2001.10422},
  year    = {2020}
}

% NAS survey (components)
@article{elsken2019neural,
  title     = {Neural architecture search: A survey},
  author    = {T. Elsken and Metzen, Jan Hendrik and F. Hutter},
  journal   = {Journal of Machine Learning Research},
  volume    = {20},
  number    = {1},
  pages     = {1997--2017},
  year      = {2019},
  publisher = {JMLR. org}
}

% cell search space
@inproceedings{zoph2018learning,
  title     = {Learning transferable architectures for scalable image recognition},
  author    = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {8697--8710},
  year      = {2018}
}

% hierarchical search space
@article{liu2017hierarchical,
  title   = {Hierarchical representations for efficient architecture search},
  author  = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
  journal = {arXiv:1711.00436},
  year    = {2017}
}

% kernel for NAS
% Arc kernel
@article{swersky2014raiders,
  title   = {Raiders of the lost architecture: Kernels for {Bayesian} optimization in conditional parameter spaces},
  author  = {K. Swersky and Duvenaud, David and J. Snoek and F. Hutter and MA. Osborne},
  journal = {arXiv:1409.4011},
  year    = {2014}
}
% NASBOT
@article{kandasamy2018neural,
  title   = {Neural architecture search with {Bayesian} optimisation and optimal transport},
  author  = {K. Kandasamy and Neiswanger, Willie and J. Schneider and Poczos, Barnabas and Xing, Eric},
  journal = {arXiv:1802.07191},
  year    = {2018}
}

% NAS with RL
@article{zoph2016neural,
  title   = {Neural architecture search with reinforcement learning},
  author  = {Zoph, Barret and Le, Quoc V},
  journal = {arXiv:1611.01578},
  year    = {2016}
}

% NAS with RE
@inproceedings{real2019regularized,
  title     = {Regularized evolution for image classifier architecture search},
  author    = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year      = {2019}
}

% Meta learn for NAS 1
@inproceedings{lian2019towards,
  title     = {Towards fast adaptation of neural architectures with meta learning},
  author    = {D. Lian and Y. Zheng and Y. Xu and Y. Lu and L. Lin and P. Zhao and J. Huang and S. Gao},
  booktitle = {International Conference on Learning Representations},
  year      = {2019}
}

% meta learn for NAS 2
@inproceedings{elsken2020meta,
  title     = {Meta-learning of neural architectures for few-shot learning},
  author    = {T. Elsken and B. Staffler and JH. Metzen and F. Hutter},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {12365--12375},
  year      = {2020}
}

% network morphism
@inproceedings{elsken2017simple,
  title     = {Simple and efficient architecture search for {CNNs}},
  author    = {T. Elsken and JH. Metzen and F. Hutter},
  booktitle = {Meta-Learning Workshop at Advances in Neural Information Processing Systems},
  year      = {2017}
}

% MO for NAS
@article{elsken2018efficient,
  title   = {Efficient multi-objective neural architecture search via Lamarckian evolution},
  author  = {T. Elsken and JH. Metzen and F. Hutter},
  journal = {arXiv:1804.09081},
  year    = {2018}
}

% one-shot, parameter sharing, ENAS
@inproceedings{pham2018efficient,
  title        = {Efficient neural architecture search via parameters sharing},
  author       = {H. Pham and M. Guan and B. Zoph and Q. Le and J. Dean},
  booktitle    = {International Conference on Machine Learning},
  year         = {2018},
}

% one-shot, parameter sharing, droppath
@inproceedings{bender2018understanding,
  title        = {Understanding and simplifying one-shot architecture search},
  author       = {G. Bender and PJ. Kindermans and B. Zoph and V. Vasudevan and Q. Le},
  booktitle    = {International Conference on Machine Learning},
  year         = {2018},
}

% DARTS
@article{liu2018darts,
  title   = {{DARTS}: Differentiable architecture search},
  author  = {H. Liu and K. Simonyan and Y. Yang},
  journal = {arXiv:1806.09055},
  year    = {2018}
}

% robust NAS
@article{zela2019understanding,
  title   = {Understanding and robustifying differentiable architecture search},
  author  = {A. Zela and T. Elsken and T. Saikia and Y. Marrakchi and T. Brox and F. Hutter},
  journal = {arXiv:1909.09656},
  year    = {2019}
}

% DARTS with adversarial training
@inproceedings{chen2020stabilizing,
  title        = {Stabilizing differentiable architecture search via perturbation-based regularization},
  author       = {X. Chen and CJ. Hsieh},
  booktitle    = {International Conference on Machine Learning},
  year         = {2020},
}

% GDAS
@inproceedings{dong2019searching,
  title     = {Searching for a robust neural architecture in four {GPU} hours},
  author    = {X. Dong and Y. Yang},
  booktitle = {Conference on Computer Vision and Pattern Recognition},
  year      = {2019}
}

% proxy NAS
@article{cai2018proxylessnas,
  title   = {{ProxylessNAS}: Direct neural architecture search on target task and hardware},
  author  = {H. Cai and L. Zhu and S. Han},
  journal = {arXiv:1812.00332},
  year    = {2018}
}

% PC DARTS
@article{xu2019pc,
  title   = {{PC-DARTS}: Partial channel connections for memory-efficient architecture search},
  author  = {Y. Xu and L. Xie and X. Zhang and X. Chen and GJ. Qi and Q. Tian and H. Xiong},
  journal = {arXiv:1907.05737},
  year    = {2019}
}

@article{saxena2016convolutional,
  title   = {Convolutional neural fabrics},
  author  = {S. Saxena and J. Verbeek},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2016}
}

% AutoRL, joint optimization of HPO and NAS
@article{runge2018learning,
  title   = {Learning to design {RNA}},
  author  = {F. Runge and D. Stoll and S. Falkner and F. Hutter},
  journal = {arXiv:1812.11951},
  year    = {2018}
}

% AutoDispNet: joint optimization of HPO and NAS
@inproceedings{saikia2019autodispnet,
  title     = {{AutoDispNet}: Improving disparity estimation with automl},
  author    = {T. Saikia and Y. Marrakchi and A. Zela and F. Hutter and T. Brox},
  booktitle = {International Conference on Computer Vision},
  year      = {2019}
}

% random search with weight sharing
@inproceedings{li2020random,
  title        = {Random search and reproducibility for neural architecture search},
  author       = {L. Li and A. Talwalkar},
  booktitle    = {Uncertainty in Artificial Intelligence},
  year         = {2020},
}

####################################################
###### section 11: dynamic algorithm configurations 
####################################################
% DAC as MDP, DAC accross datasets
@incollection{biedenkapp2020dynamic,
  title     = {Dynamic algorithm configuration: foundation of a new meta-algorithmic framework},
  author    = {A. Biedenkapp and HF. Bozkurt and T. Eimer and F. Hutter and M. Lindauer},
  booktitle = {European Conference on Artificial Intelligence},
  year      = {2020}
}

% curriculum learning
@inproceedings{bengio2009curriculum,
  title     = {Curriculum learning},
  author    = {Y. Bengio and J. Louradour and R. Collobert and J. Weston},
  booktitle = {International Conference on Machine Learning},
  year      = {2009}
}

% self-paced learning
@article{kumar2010self,
  title   = {Self-paced learning for latent variable models},
  author  = {M. Kumar and B. Packer and D. Koller},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2010}
}

% reinforcement learning for step size scheduling
@inproceedings{daniel2016learning,
  title     = {Learning step size controllers for robust neural network training},
  author    = {C. Daniel and J. Taylor and S. Nowozin},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year      = {2016}
}

% population-based training
@article{jaderberg2017population,
  title   = {Population based training of neural networks},
  author  = {M. Jaderberg and V. Dalibard and S. Osindero and WM. Czarnecki and J. Donahue and A. Razavi and O. Vinyals and T. Green and I. Dunning and K. Simonyan and others},
  journal = {arXiv:1711.09846},
  year    = {2017}
}

% population-based training
@inproceedings{liang2021regularized,
  title     = {Regularized evolutionary population-based training},
  author    = {J. Liang and S. Gonzalez and H. Shahrzad and R. Miikkulainen},
  booktitle = {Genetic and Evolutionary Computation Conference},
  year      = {2021}
}

% population-based training: parallel settings
@article{parker2020provably,
  title   = {Provably efficient online hyperparameter optimization with population-based bandits},
  author  = {J. Parker-Holder and V. Nguyen and S. Roberts},
  journal = {arXiv:2002.02518},
  year    = {2020}
}

% learning black-box optimizers
% --> andrychowicz2016learning

% DAC for BB optimizer
@inproceedings{shala2020learning,
  title        = {Learning step-size adaptation in {CMA-ES}},
  author       = {G. Shala and A. Biedenkapp and N. Awad and S. Adriaensen and M. Lindauer and F. Hutter},
  booktitle    = {International Conference on Parallel Problem Solving from Nature},
  year         = {2020},
}

% L2L by gradient descent (L2L for LSTM)
% --> andrychowicz2016learning

% Neural acquisition function
% --> volpp2019meta

% learning to optimize via reinforcement learning
% --> li2016learning

####################################################
###### section 12: Interpretability 
####################################################
% various visualization
@article{lindauer2019boah,
  title   = {{BOAH}: A tool suite for multi-fidelity {Bayesian} optimization \& analysis of hyperparameters},
  author  = {M. Lindauer and K. Eggensperger and M. Feurer and A. Biedenkapp and J. Marben and M{\"u}ller, Philipp and F. Hutter},
  journal = {arXiv:1908.06756},
  year    = {2019}
}

% 2d land-scape
@article{lindauer2019towards,
  title   = {Towards Assessing the Impact of {Bayesian} Optimization's Own Hyperparameters},
  author  = {M. Lindauer and M. Feurer and K. Eggensperger and A. Biedenkapp and F. Hutter},
  journal = {arXiv:1908.06674},
  year    = {2019}
}

% parallel coordinate plot
@inproceedings{golovin2017google,
  title     = {{Google Vizier}: A service for black-box optimization},
  author    = {Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, D},
  booktitle = {Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages     = {1487--1495},
  year      = {2017}
}

% various visualization, local hyperparameter importance
@inproceedings{biedenkapp2018cave,
  title        = {{CAVE}: Configuration assessment, visualization and evaluation},
  author       = {A. Biedenkapp and J. Marben and M. Lindauer and F. Hutter},
  booktitle    = {International Conference on Learning and Intelligent Optimization},
  pages        = {115--130},
  year         = {2018},
  organization = {Springer}
}

% forward selection of hyperparameters for hyperparameter importance
@inproceedings{hutter2013identifying,
  title        = {Identifying key algorithm parameters and instance features using forward selection},
  author       = {F. Hutter and H. Hoos H and K. Leyton-Brown},
  booktitle    = {International Conference on Learning and Intelligent Optimization},
  pages        = {364--381},
  year         = {2013},
  organization = {Springer}
}

% ablation study
@inproceedings{biedenkapp2017efficient,
  title     = {Efficient parameter importance analysis via ablation with surrogates},
  author    = {A. Biedenkapp and M. Lindauer and K. Eggensperger and F. Hutter and C. Fawcett and H. Hoos},
  booktitle = {AAAI Conference on Artificial Intelligence},
  volume    = {31},
  number    = {1},
  year      = {2017}
}

% ablation study: original paper
@article{fawcett2016analysing,
  title     = {Analysing differences between algorithm configurations through ablation},
  author    = {C. Fawcett and H. Hoos H},
  journal   = {Journal of Heuristics},
  volume    = {22},
  number    = {4},
  pages     = {431--458},
  year      = {2016},
  publisher = {Springer}
}

% global hyperparameter importance
@inproceedings{hutter2014efficient,
  title     = {An efficient approach for assessing hyperparameter importance},
  author    = {F. Hutter and H. Hoos and K. Leyton-Brown},
  booktitle = {International Conference on Machine Learning},
  year      = {2014}
}

% The usage of global hyperparameter importance
@inproceedings{van2018hyperparameter,
  title     = {Hyperparameter importance across datasets},
  author    = {JN. Van Rijn and F. Hutter},
  booktitle = {International Conference on Knowledge Discovery \& Data Mining},
  year      = {2018}
}
